import re
import argparse
from dataclasses import dataclass
from typing import List, Tuple
from collections import defaultdict
from pathlib import Path

import pdfplumber
import pandas as pd

# OCR fallback (optional; safe to import even if not used)
try:
    import pytesseract
    from pdf2image import convert_from_path
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False


# ----------------------------
# Config / Patterns
# ----------------------------
CODE_RE = re.compile(r"^(?:\d{5}|[A-Z]\d{4})$")  # CPT (5 digits) or HCPCS (1 letter + 4 digits)
MONEY_RE = re.compile(r"^\$?\d+(?:\.\d{1,4})?$") # price-like numbers ($ optional)
ONLY_MONEY_RE = re.compile(r"^\$\d+(?:\.\d{1,4})?$")
ONLY_NUMERIC_RE = re.compile(r"^\d+(?:\.\d{1,4})?$")


@dataclass
class PageSpec:
    page: int             # 1-based page number
    is_cont: bool = False # whether it's a continuation page (same logic; just clarity)
    rate_mode: str = "auto"  # 'auto' (with or without $), 'dollar', or 'numeric'


def parse_pages_arg(pages_str: str) -> List[PageSpec]:
    """
    Parse user input like:
      "21, 22, 24; 25 CONT; 158 NODOLLAR; 159 CONT NODOLLAR"
    Meaning of tokens (case-insensitive):
      CONT        -> continuation page
      DOLLAR      -> rate must have '$'
      NODOLLAR    -> rate can be numeric without '$'
      AUTO        -> allow $ or not (default)
    """
    out: List[PageSpec] = []
    if not pages_str:
        return out

    # split by comma or semicolon
    chunks = re.split(r"[;,]\s*", pages_str.strip())

    for ch in chunks:
        if not ch:
            continue
        tokens = ch.strip().split()
        # first token must be page number
        try:
            pg = int(tokens[0])
        except ValueError:
            raise ValueError(f"Invalid page entry: {ch!r}")

        is_cont = False
        rate_mode = "auto"
        for t in tokens[1:]:
            t_upper = t.strip().upper()
            if t_upper == "CONT":
                is_cont = True
            elif t_upper in ("DOLLAR", "WITH$"):
                rate_mode = "dollar"
            elif t_upper in ("NODOLLAR", "NUMERIC"):
                rate_mode = "numeric"
            elif t_upper == "AUTO":
                rate_mode = "auto"
            else:
                # ignore unknown tokens silently
                pass

        out.append(PageSpec(page=pg, is_cont=is_cont, rate_mode=rate_mode))
    return out


def extract_tables(page) -> List[List[str]]:
    rows = []
    tables = page.extract_tables() or []
    for tbl in tables:
        for row in tbl:
            cells = [(c or "").strip() for c in row]
            if any(cells):
                # remove empties to simplify token scanning
                rows.append([c for c in cells if c != ""])
    return rows


def extract_words(page) -> List[List[str]]:
    """
    Reconstruct lines from words (works when tables fail).
    """
    words = page.extract_words(
        x_tolerance=2,
        y_tolerance=3,
        keep_blank_chars=False,
        use_text_flow=True
    ) or []

    lines = defaultdict(list)
    for w in words:
        ykey = round(w["top"])
        lines[ykey].append((w["x0"], w["text"]))

    rows = []
    for y in sorted(lines.keys()):
        toks = [t for _, t in sorted(lines[y], key=lambda z: z[0])]
        cleaned = []
        for t in toks:
            cleaned.extend([s for s in re.split(r"\s+", t.strip()) if s])
        if cleaned:
            rows.append(cleaned)
    return rows


def extract_ocr(pdf_path: str, page_index_0: int) -> List[List[str]]:
    """
    OCR fallback: rasterize page and run Tesseract; returns simple tokenized lines.
    """
    if not OCR_AVAILABLE:
        return []
    images = convert_from_path(pdf_path, first_page=page_index_0+1, last_page=page_index_0+1, dpi=300)
    if not images:
        return []
    img = images[0]
    txt = pytesseract.image_to_string(img)
    rows = []
    for line in txt.splitlines():
        line = line.strip()
        if not line:
            continue
        rows.append([t for t in re.split(r"\s+", line) if t])
    return rows


def slice_mods(tokens: List[str]) -> Tuple[str, str, str, str]:
    """
    Modifiers are built from short (<=2 char) non-price tokens (ignoring bare '$'),
    concatenated and split into pairs: Mod1..Mod4.
    """
    mod_chars = [t for t in tokens if len(t.strip()) <= 2 and t.strip() != "$" and not MONEY_RE.match(t.strip())]
    mod_concat = "".join(mod_chars)
    mods = [mod_concat[i:i+2] for i in range(0, min(8, len(mod_concat)), 2)]
    while len(mods) < 4:
        mods.append("")
    return mods[0], mods[1], mods[2], mods[3]


def detect_rate(tokens: List[str], rate_mode: str) -> str:
    """
    rate_mode:
      - 'dollar': require tokens like $12.34
      - 'numeric': allow 12.34 (no $)
      - 'auto': allow either
    Returns rate without the leading '$' (if present), or "" if not found.
    """
    for t in tokens:
        s = t.strip()
        if rate_mode == "dollar":
            if ONLY_MONEY_RE.match(s):
                return s.lstrip("$")
        elif rate_mode == "numeric":
            if ONLY_NUMERIC_RE.match(s) or ONLY_MONEY_RE.match(s):
                return s.lstrip("$")
        else:  # auto
            if MONEY_RE.match(s):
                return s.lstrip("$")
    return ""


def parse_rows(rows: List[List[str]], page_num: int, rate_mode: str) -> List[dict]:
    """
    Scan each row; for each token that looks like a Code, collect following tokens
    until the next code or a heading-ish token (contains 'Code' or 'Rate').
    Build records with Code, Mod1..4, Rate.
    """
    out = []
    for row in rows:
        N = len(row)
        i = 0
        while i < N:
            tok = row[i].strip()
            if CODE_RE.match(tok):
                code = tok
                after = []
                j = i + 1
                while j < N:
                    t = row[j].strip()
                    if CODE_RE.match(t) or "Code" in t or "Rate" in t:
                        break
                    after.append(t)
                    j += 1

                mod1, mod2, mod3, mod4 = slice_mods(after)
                rate = detect_rate(after, rate_mode)

                # keep only if we actually captured table-like data
                if rate or any([mod1, mod2, mod3, mod4]):
                    out.append({
                        "Code": code,
                        "Mod1": mod1,
                        "Mod2": mod2,
                        "Mod3": mod3,
                        "Mod4": mod4,
                        "Rate": rate,
                        "Page": page_num
                    })
                i = j
            else:
                i += 1
    return out


def extract_page(pdf_path: str, page_num_1based: int, rate_mode: str, ocr_policy: str) -> List[dict]:
    """
    ocr_policy: 'never' | 'auto' | 'always'
    """
    page_index_0 = page_num_1based - 1
    rows: List[List[str]] = []

    with pdfplumber.open(pdf_path) as pdf:
        page = pdf.pages[page_index_0]

        # Try table-based first
        rows = extract_tables(page)

        # If weak/empty, try word reconstruction
        if not rows or all(len(r) <= 1 for r in rows):
            rows = extract_words(page)

    # Parse
    recs = parse_rows(rows, page_num_1based, rate_mode)

    # OCR fallback if needed
    need_ocr = (ocr_policy in ("auto", "always")) and (not recs or any(r["Rate"] == "" for r in recs))
    if ocr_policy == "always" or (ocr_policy == "auto" and need_ocr):
        ocr_rows = extract_ocr(pdf_path, page_index_0)
        if ocr_rows:
            # Only replace missing items or empty set; simple approach: re-parse OCR and merge/override
            ocr_recs = parse_rows(ocr_rows, page_num_1based, rate_mode)
            if recs:
                # fill missing rates from OCR where codes/rows align by order
                k = min(len(recs), len(ocr_recs))
                for i in range(k):
                    if not recs[i]["Rate"] and ocr_recs[i]["Rate"]:
                        recs[i]["Rate"] = ocr_recs[i]["Rate"]
            else:
                recs = ocr_recs

    return recs


def main():
    ap = argparse.ArgumentParser(description="Extract code/modifier/rate rows from a PDF into Excel.")
    ap.add_argument("--pdf", required=True, help="Path to the PDF file.")
    ap.add_argument("--pages", required=True,
                    help=("Page list, e.g.: "
                          "\"21, 22, 24; 25 CONT; 158 NODOLLAR; 159 CONT NODOLLAR\""))
    ap.add_argument("--out", default="extracted_output.xlsx", help="Output Excel file path.")
    ap.add_argument("--ocr", choices=("never", "auto", "always"), default="auto",
                    help="OCR fallback policy (default: auto).")
    args = ap.parse_args()

    pdf_path = args.pdf
    page_specs = parse_pages_arg(args.pages)
    if not page_specs:
        print("No valid pages parsed from --pages.")
        return

    all_rows = []
    for spec in page_specs:
        recs = extract_page(pdf_path, spec.page, spec.rate_mode, args.ocr)
        all_rows.extend(recs)

    if not all_rows:
        print("No rows extracted. Try --ocr always, or adjust pages/rate modes.")
        return

    df = pd.DataFrame(all_rows, columns=["Code", "Mod1", "Mod2", "Mod3", "Mod4", "Rate", "Page"])
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    df.to_excel(args.out, index=False)
    print(f"âœ… Done. Wrote {len(df)} rows to {args.out}")


if __name__ == "__main__":
    main()
